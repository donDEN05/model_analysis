import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch
import numpy as np
from collections import defaultdict
import inspect

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
plt.rcParams['figure.figsize'] = (20, 14)
plt.rcParams['font.size'] = 9

print("="*80)
print("üîç –ê–ù–ê–õ–ò–ó –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ú–û–î–ï–õ–ò FINBERT")
print("="*80)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ProsusAI/finbert...")
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained(
    "ProsusAI/finbert", 
    num_labels=11,
    ignore_mismatched_sizes=True  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–∞ classifier
)
model.eval()

print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
def analyze_model_structure(model, prefix=""):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏"""
    layers_info = []
    total_params = 0
    
    for name, module in model.named_children():
        module_type = type(module).__name__
        num_params = sum(p.numel() for p in module.parameters())
        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
        
        layer_info = {
            'name': name,
            'type': module_type,
            'params': num_params,
            'trainable_params': trainable_params,
            'full_path': f"{prefix}.{name}" if prefix else name
        }
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–æ–¥—ã –º–æ–¥—É–ª—è
        methods = [method for method in dir(module) 
                  if not method.startswith('_') and callable(getattr(module, method, None))]
        layer_info['methods'] = methods[:10]  # –ü–µ—Ä–≤—ã–µ 10 –º–µ—Ç–æ–¥–æ–≤
        
        layers_info.append(layer_info)
        total_params += num_params
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
        if len(list(module.children())) > 0:
            sub_layers = analyze_model_structure(module, f"{prefix}.{name}" if prefix else name)
            layers_info.extend(sub_layers)
    
    return layers_info

# –°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
print("üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏...")
layers_info = analyze_model_structure(model)

# –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
total_params = sum(l['params'] for l in layers_info)
trainable_params = sum(l['trainable_params'] for l in layers_info)
layer_types = defaultdict(int)
for layer in layers_info:
    layer_types[layer['type']] += 1

print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,} ({total_params/1e6:.2f}M)")
print(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
print(f"   –í—Å–µ–≥–æ —Å–ª–æ–µ–≤: {len(layers_info)}")
print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ª–æ–µ–≤: {len(layer_types)}")

print(f"\nüìã –¢–ò–ü–´ –°–õ–û–ï–í:")
for layer_type, count in sorted(layer_types.items(), key=lambda x: -x[1]):
    print(f"   {layer_type}: {count}")

# –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ—è—Ö
print(f"\n{'='*80}")
print("üî¨ –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–õ–û–Ø–•")
print(f"{'='*80}")

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
bert_layers = [l for l in layers_info if 'bert' in l['name'].lower() or 'encoder' in l['name'].lower()]
classifier_layers = [l for l in layers_info if 'classifier' in l['name'].lower() or 'dropout' in l['name'].lower()]
embedding_layers = [l for l in layers_info if 'embedding' in l['name'].lower()]

print(f"\nüß† BERT Encoder —Å–ª–æ–∏: {len(bert_layers)}")
print(f"üìä Classifier —Å–ª–æ–∏: {len(classifier_layers)}")
print(f"üî§ Embedding —Å–ª–æ–∏: {len(embedding_layers)}")

# –í—ã–≤–æ–¥ —Ç–æ–ø-10 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö —Å–ª–æ–µ–≤
print(f"\nüìä –¢–û–ü-10 –°–õ–û–ï–í –ü–û –ö–û–õ–ò–ß–ï–°–¢–í–£ –ü–ê–†–ê–ú–ï–¢–†–û–í:")
sorted_layers = sorted(layers_info, key=lambda x: -x['params'])[:10]
for i, layer in enumerate(sorted_layers, 1):
    print(f"   {i:2d}. {layer['full_path']:50s} | {layer['type']:30s} | {layer['params']:>12,} params")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 1: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
print(f"\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")

fig = plt.figure(figsize=(24, 16))
gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

# –ì—Ä–∞—Ñ–∏–∫ 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Ç–∏–ø–∞–º —Å–ª–æ–µ–≤
ax1 = fig.add_subplot(gs[0, 0])
layer_params = defaultdict(int)
for layer in layers_info:
    layer_params[layer['type']] += layer['params']

sorted_types = sorted(layer_params.items(), key=lambda x: -x[1])[:15]
types_names = [t[0] for t in sorted_types]
types_params = [t[1] for t in sorted_types]

colors = plt.cm.viridis(np.linspace(0, 1, len(types_names)))
bars = ax1.barh(range(len(types_names)), [p/1e6 for p in types_params], color=colors)
ax1.set_yticks(range(len(types_names)))
ax1.set_yticklabels(types_names, fontsize=8)
ax1.set_xlabel('–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–∏–ª–ª–∏–æ–Ω—ã)', fontsize=10, fontweight='bold')
ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Ç–∏–ø–∞–º —Å–ª–æ–µ–≤', fontsize=12, fontweight='bold')
ax1.grid(axis='x', alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for i, (bar, params) in enumerate(zip(bars, types_params)):
    ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
             f'{params/1e6:.2f}M', va='center', fontsize=8)

# –ì—Ä–∞—Ñ–∏–∫ 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –ø–æ —Ç–∏–ø–∞–º
ax2 = fig.add_subplot(gs[0, 1])
type_counts = sorted(layer_types.items(), key=lambda x: -x[1])[:15]
type_names = [t[0] for t in type_counts]
type_counts_vals = [t[1] for t in type_counts]

colors2 = plt.cm.plasma(np.linspace(0, 1, len(type_names)))
bars2 = ax2.bar(range(len(type_names)), type_counts_vals, color=colors2)
ax2.set_xticks(range(len(type_names)))
ax2.set_xticklabels(type_names, rotation=45, ha='right', fontsize=8)
ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤', fontsize=10, fontweight='bold')
ax2.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –ø–æ —Ç–∏–ø–∞–º', fontsize=12, fontweight='bold')
ax2.grid(axis='y', alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for bar, count in zip(bars2, type_counts_vals):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
             str(count), ha='center', va='bottom', fontsize=8)

# –ì—Ä–∞—Ñ–∏–∫ 3: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–µ–º–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
ax3 = fig.add_subplot(gs[1, :])
ax3.set_xlim(0, 10)
ax3.set_ylim(0, 8)
ax3.axis('off')
ax3.set_title('–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–µ–º–∞ FinBERT', fontsize=14, fontweight='bold', pad=20)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
components = [
    {'name': 'Input\n(Text)', 'x': 1, 'y': 4, 'color': '#FF6B6B', 'width': 0.8, 'height': 1},
    {'name': 'Tokenizer', 'x': 2.5, 'y': 4, 'color': '#4ECDC4', 'width': 0.8, 'height': 1},
    {'name': 'Embeddings\n(Word + Position + Token)', 'x': 4, 'y': 4, 'color': '#95E1D3', 'width': 1.2, 'height': 1},
    {'name': 'BERT Encoder\n(12 Transformer Layers)', 'x': 6, 'y': 4, 'color': '#F38181', 'width': 1.5, 'height': 2},
    {'name': 'Pooler\n(CLS Token)', 'x': 8, 'y': 5, 'color': '#AA96DA', 'width': 0.8, 'height': 0.6},
    {'name': 'Dropout', 'x': 8, 'y': 3.5, 'color': '#FCBAD3', 'width': 0.8, 'height': 0.6},
    {'name': 'Classifier\n(Linear Layer)', 'x': 9.5, 'y': 4, 'color': '#FFD93D', 'width': 0.8, 'height': 1},
]

# –†–∏—Å—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
for comp in components:
    box = FancyBboxPatch(
        (comp['x'], comp['y'] - comp['height']/2),
        comp['width'], comp['height'],
        boxstyle="round,pad=0.1",
        facecolor=comp['color'],
        edgecolor='black',
        linewidth=1.5,
        alpha=0.7
    )
    ax3.add_patch(box)
    ax3.text(comp['x'] + comp['width']/2, comp['y'], comp['name'],
             ha='center', va='center', fontsize=9, fontweight='bold')

# –†–∏—Å—É–µ–º —Å—Ç—Ä–µ–ª–∫–∏
arrows = [
    (1.8, 4, 2.5, 4),
    (3.3, 4, 4, 4),
    (5.2, 4, 6, 4),
    (7.5, 4.3, 8, 4.3),
    (7.5, 3.7, 8, 3.7),
    (8.8, 4, 9.5, 4),
]

for x1, y1, x2, y2 in arrows:
    arrow = FancyArrowPatch(
        (x1, y1), (x2, y2),
        arrowstyle='->', lw=2, color='black', alpha=0.6
    )
    ax3.add_patch(arrow)

# –ì—Ä–∞—Ñ–∏–∫ 4: –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ BERT Encoder
ax4 = fig.add_subplot(gs[2, 0])
ax4.set_xlim(0, 10)
ax4.set_ylim(0, 6)
ax4.axis('off')
ax4.set_title('–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Transformer Layer', fontsize=12, fontweight='bold', pad=15)

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Transformer —Å–ª–æ—è
transformer_components = [
    {'name': 'Multi-Head\nAttention', 'x': 2, 'y': 4.5, 'color': '#FF6B9D', 'w': 1.2, 'h': 0.8},
    {'name': 'Add & Norm', 'x': 3.5, 'y': 4.5, 'color': '#C44569', 'w': 0.8, 'h': 0.8},
    {'name': 'Feed Forward\n(2 Linear)', 'x': 5, 'y': 4.5, 'color': '#F8B500', 'w': 1.2, 'h': 0.8},
    {'name': 'Add & Norm', 'x': 6.5, 'y': 4.5, 'color': '#C44569', 'w': 0.8, 'h': 0.8},
    {'name': 'Input', 'x': 0.5, 'y': 4.5, 'color': '#95E1D3', 'w': 0.8, 'h': 0.8},
    {'name': 'Output', 'x': 7.8, 'y': 4.5, 'color': '#95E1D3', 'w': 0.8, 'h': 0.8},
]

for comp in transformer_components:
    box = FancyBboxPatch(
        (comp['x'], comp['y'] - comp['h']/2),
        comp['w'], comp['h'],
        boxstyle="round,pad=0.05",
        facecolor=comp['color'],
        edgecolor='black',
        linewidth=1.2,
        alpha=0.7
    )
    ax4.add_patch(box)
    ax4.text(comp['x'] + comp['w']/2, comp['y'], comp['name'],
             ha='center', va='center', fontsize=8, fontweight='bold')

# –°—Ç—Ä–µ–ª–∫–∏ –¥–ª—è transformer
transformer_arrows = [
    (1.3, 4.5, 2, 4.5),
    (3.2, 4.5, 3.5, 4.5),
    (4.3, 4.5, 5, 4.5),
    (6.2, 4.5, 6.5, 4.5),
    (7.3, 4.5, 7.8, 4.5),
    # Residual connections
    (0.9, 4.5, 0.9, 3.5), (0.9, 3.5, 3.5, 3.5), (3.5, 3.5, 3.5, 4.1),  # First residual
    (4.7, 4.5, 4.7, 2.5), (4.7, 2.5, 6.5, 2.5), (6.5, 2.5, 6.5, 4.1),  # Second residual
]

for coords in transformer_arrows:
    if len(coords) == 4:
        arrow = FancyArrowPatch(
            (coords[0], coords[1]), (coords[2], coords[3]),
            arrowstyle='->', lw=1.5, color='#2C3E50', alpha=0.5, 
            connectionstyle="arc3,rad=0.1" if abs(coords[1] - coords[3]) > 0.1 else None
        )
        ax4.add_patch(arrow)

# –ì—Ä–∞—Ñ–∏–∫ 5: –ú–µ—Ç–æ–¥—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
ax5 = fig.add_subplot(gs[2, 1])
ax5.axis('off')
ax5.set_title('–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –º–æ–¥–µ–ª–∏', fontsize=12, fontweight='bold', pad=15)

# –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–æ–¥—ã –º–æ–¥–µ–ª–∏
model_methods = [method for method in dir(model) 
                if not method.startswith('_') and callable(getattr(model, method, None))]

# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
forward_methods = [m for m in model_methods if 'forward' in m.lower() or 'call' in m.lower()]
get_methods = [m for m in model_methods if m.startswith('get')]
set_methods = [m for m in model_methods if m.startswith('set')]
other_methods = [m for m in model_methods if m not in forward_methods + get_methods + set_methods][:15]

methods_text = "–û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´:\n\n"
methods_text += "üîπ Forward –º–µ—Ç–æ–¥—ã:\n"
for m in forward_methods[:5]:
    methods_text += f"   ‚Ä¢ {m}\n"

methods_text += "\nüîπ Get –º–µ—Ç–æ–¥—ã:\n"
for m in get_methods[:5]:
    methods_text += f"   ‚Ä¢ {m}\n"

methods_text += "\nüîπ Set –º–µ—Ç–æ–¥—ã:\n"
for m in set_methods[:5]:
    methods_text += f"   ‚Ä¢ {m}\n"

methods_text += "\nüîπ –î—Ä—É–≥–∏–µ –≤–∞–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã:\n"
for m in other_methods[:10]:
    methods_text += f"   ‚Ä¢ {m}\n"

ax5.text(0.05, 0.95, methods_text, transform=ax5.transAxes,
         fontsize=9, verticalalignment='top', family='monospace',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏)
import os
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
output_path = os.path.join(BASE_DIR, "model_visualization.png")
plt.savefig(output_path, dpi=300, bbox_inches='tight')
print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å
print(f"\n{'='*80}")
print("üìù –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–û–ú–ü–û–ù–ï–ù–¢–ê–•")
print(f"{'='*80}")

# –ê–Ω–∞–ª–∏–∑ BERT encoder
bert_model = model.bert if hasattr(model, 'bert') else None
if bert_model:
    print(f"\nüß† BERT Encoder:")
    if hasattr(bert_model, 'encoder'):
        encoder = bert_model.encoder
        if hasattr(encoder, 'layer'):
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Transformer —Å–ª–æ–µ–≤: {len(encoder.layer)}")
            if len(encoder.layer) > 0:
                first_layer = encoder.layer[0]
                print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è:")
                for name, module in first_layer.named_children():
                    print(f"      - {name}: {type(module).__name__}")

# –ê–Ω–∞–ª–∏–∑ Embeddings
if bert_model and hasattr(bert_model, 'embeddings'):
    embeddings = bert_model.embeddings
    print(f"\nüî§ Embeddings:")
    for name, module in embeddings.named_children():
        print(f"   - {name}: {type(module).__name__}")

# –ê–Ω–∞–ª–∏–∑ Classifier
if hasattr(model, 'classifier'):
    classifier = model.classifier
    print(f"\nüìä Classifier:")
    if isinstance(classifier, nn.Sequential):
        for i, module in enumerate(classifier):
            print(f"   Layer {i}: {type(module).__name__}")
            if hasattr(module, 'in_features') and hasattr(module, 'out_features'):
                print(f"      Input: {module.in_features}, Output: {module.out_features}")
    else:
        print(f"   Type: {type(classifier).__name__}")
        if hasattr(classifier, 'in_features') and hasattr(classifier, 'out_features'):
            print(f"      Input: {classifier.in_features}, Output: {classifier.out_features}")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ä–∞—Ö
print(f"\nüìè –†–ê–ó–ú–ï–†–´ –ú–û–î–ï–õ–ò:")
if bert_model:
    if hasattr(bert_model, 'config'):
        config = bert_model.config
        print(f"   Hidden size: {config.hidden_size}")
        print(f"   Number of attention heads: {config.num_attention_heads}")
        print(f"   Number of hidden layers: {config.num_hidden_layers}")
        print(f"   Intermediate size: {config.intermediate_size}")
        print(f"   Max position embeddings: {config.max_position_embeddings}")
        print(f"   Vocabulary size: {config.vocab_size}")

print(f"\n{'='*80}")
print("‚úÖ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
print(f"{'='*80}")
print(f"\nüìÅ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
print(f"üìä –í—Å–µ–≥–æ —Å–ª–æ–µ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(layers_info)}")
print(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {total_params/1e6:.2f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

plt.show()

