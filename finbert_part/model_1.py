import os
import sys
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification




# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data.csv")

try:
    data = pd.read_csv(DATA_PATH)
    print(f'‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_columns = ['text', 'emotion']
    missing_columns = [col for col in required_columns if col not in data.columns]
    if missing_columns:
        print(f"‚ùå –û—à–∏–±–∫–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        sys.exit(1)
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
    sys.exit(1)


device = "cpu"
use_cuda_env = torch.cuda.is_available()

# –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
cuda_available = torch.cuda.is_available()
if cuda_available:
    cuda_device_count = torch.cuda.device_count()
    cuda_device_name = torch.cuda.get_device_name(0)
    cuda_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
    
    print("="*60)
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA:")
    print(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: ‚úÖ")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {cuda_device_count}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ 0: {cuda_device_name}")
    print(f"   –û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {cuda_memory_total:.2f} GB")
    print(f"   –í–µ—Ä—Å–∏—è CUDA: {torch.version.cuda}")
    print(f"   –í–µ—Ä—Å–∏—è cuDNN: {torch.backends.cudnn.version()}")
    print("="*60)
    
    # –ï—Å–ª–∏ USE_CUDA —è–≤–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ "false", –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
    if use_cuda_env == False:
        device = "cpu"
        print("‚ö†Ô∏è  CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (USE_CUDA=false)")
    else:
        device = 'cuda'
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {cuda_device_name}")
else:
    print("="*60)
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA:")
    print("   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: ‚ùå")
    print("   –ü—Ä–∏—á–∏–Ω–∞: GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –∏–ª–∏ CUDA –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    print("="*60)
    print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω)")
    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
    print("   - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA")
    print("   - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
    print("   - GPU –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA")
    print("="*60)


tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")