import os
import sys
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import mlflow
import mlflow.pytorch
import numpy as np
from datetime import datetime
import pickle
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ—Å—Ç–∏)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data.csv")
MODEL_SAVE_PATH = os.path.join(BASE_DIR, "finbert_finetuned")
MLFLOW_EXPERIMENT_NAME = "finbert_emotion_classification"
MLFLOW_TRACKING_URI = os.path.join(BASE_DIR, "mlruns")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("="*60)
print("üìä –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("="*60)
try:
    data = pd.read_csv(DATA_PATH)
    print(f'‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {data.shape[0]} –∑–∞–ø–∏—Å–µ–π, {data.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_columns = ['text', 'emotion']
    missing_columns = [col for col in required_columns if col not in data.columns]
    if missing_columns:
        print(f"‚ùå –û—à–∏–±–∫–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        sys.exit(1)
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
    sys.exit(1)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nüñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("\n" + "="*60)
print("üîß –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
print("="*60)

# –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
data = data.dropna(subset=['text', 'emotion'])
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(data)} –∑–∞–ø–∏—Å–µ–π")

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['emotion'])
num_labels = len(label_encoder.classes_)
print(f"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_labels}")
print(f"   –ö–ª–∞—Å—Å—ã: {list(label_encoder.classes_)}")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42, stratify=data['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
print(f"   Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π ({len(train_df)/len(data)*100:.1f}%)")
print(f"   Validation: {len(val_df)} –∑–∞–ø–∏—Å–µ–π ({len(val_df)/len(data)*100:.1f}%)")
print(f"   Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π ({len(test_df)/len(data)*100:.1f}%)")

# –°–æ–∑–¥–∞–Ω–∏–µ Dataset –∫–ª–∞—Å—Å–∞
class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
print("\n" + "="*60)
print("ü§ñ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò")
print("="*60)
print("–ó–∞–≥—Ä—É–∑–∫–∞ ProsusAI/finbert...")

tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
# –ò—Å–ø–æ–ª—å–∑—É–µ–º ignore_mismatched_sizes=True, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ BERT
# –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π classifier head —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤
# –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π finbert –æ–±—É—á–µ–Ω –Ω–∞ 3 –∫–ª–∞—Å—Å–∞ (positive/negative/neutral),
# –Ω–æ –Ω–∞–º –Ω—É–∂–Ω–æ 11 –∫–ª–∞—Å—Å–æ–≤ —ç–º–æ—Ü–∏–π, –ø–æ—ç—Ç–æ–º—É classifier –±—É–¥–µ—Ç –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
print(f"‚ö†Ô∏è  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç 3 –∫–ª–∞—Å—Å–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π classifier –¥–ª—è {num_labels} –∫–ª–∞—Å—Å–æ–≤")
model = AutoModelForSequenceClassification.from_pretrained(
    "ProsusAI/finbert",
    num_labels=num_labels,
    problem_type="single_label_classification",
    ignore_mismatched_sizes=True  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–∞ classifier
)
model.to(device)

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è {num_labels} –∫–ª–∞—Å—Å–æ–≤")

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
train_dataset = EmotionDataset(train_df['text'], train_df['label'], tokenizer)
val_dataset = EmotionDataset(val_df['text'], val_df['label'], tokenizer)
test_dataset = EmotionDataset(test_df['text'], test_df['label'], tokenizer)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLFlow –ø–æ –±–∞—Ç—á–∞–º
class MLFlowLoggingCallback(TrainerCallback):
    """Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –≤ MLFlow –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –∏ –±–∞—Ç—á–µ"""
    
    def __init__(self):
        self.step = 0
        self.epoch = 0
        
    def on_train_step_end(self, args, state, control, model=None, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è (–∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞)"""
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å loss –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        batch_loss = None
        
        # –°–ø–æ—Å–æ–± 1: –∏–∑ log_history (–µ—Å–ª–∏ —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω)
        if hasattr(state, 'log_history') and len(state.log_history) > 0:
            last_log = state.log_history[-1]
            if 'loss' in last_log:
                batch_loss = last_log['loss']
        
        # –°–ø–æ—Å–æ–± 2: –∏–∑ kwargs (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –Ω–∞–ø—Ä—è–º—É—é)
        if batch_loss is None and 'loss' in kwargs:
            batch_loss = kwargs['loss']
        
        # –õ–æ–≥–∏—Ä—É–µ–º loss –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –≤ MLFlow
        if batch_loss is not None:
            try:
                mlflow.log_metric('batch_loss', float(batch_loss), step=state.global_step)
            except Exception as e:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ
                pass
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ (–∫–∞–∂–¥—ã–π logging_steps)"""
        if logs is not None:
            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ MLFlow
            metrics_to_log = {}
            
            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π loss (—ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ logging_steps –±–∞—Ç—á–µ–π)
            if 'loss' in logs:
                metrics_to_log['train_loss_avg'] = logs['loss']
                
            # –õ–æ–≥–∏—Ä—É–µ–º learning rate –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if 'learning_rate' in logs:
                metrics_to_log['learning_rate'] = logs['learning_rate']
                
            # –õ–æ–≥–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            for key, value in logs.items():
                if key not in ['loss', 'learning_rate', 'epoch'] and isinstance(value, (int, float)):
                    metrics_to_log[f'train_{key}'] = value
            
            if metrics_to_log:
                mlflow.log_metrics(metrics_to_log, step=state.global_step)
                
    def on_epoch_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏"""
        self.epoch = state.epoch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLFlow
print("\n" + "="*60)
print("üìà –ù–ê–°–¢–†–û–ô–ö–ê MLFLOW")
print("="*60)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º tracking URI –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
if os.name == 'nt':  # Windows
    # –ù–∞ Windows –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç file:///D:/path/to/mlruns (—Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤–æ–π –¥–∏—Å–∫–∞)
    abs_path = os.path.abspath(MLFLOW_TRACKING_URI)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—É—Ç—å: D:\path\to\mlruns -> D:/path/to/mlruns
    uri_path = abs_path.replace('\\', '/')
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –±—É–∫–≤–∞ –¥–∏—Å–∫–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è
    if len(uri_path) > 1 and uri_path[1] == ':':
        uri_path = uri_path[0].upper() + uri_path[1:]
    tracking_uri = f"file:///{uri_path}"
else:  # Unix/Linux/Mac
    tracking_uri = f"file://{os.path.abspath(MLFLOW_TRACKING_URI)}"
mlflow.set_tracking_uri(tracking_uri)
mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
print(f"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç MLFlow: {MLFLOW_EXPERIMENT_NAME}")
print(f"‚úÖ MLFlow tracking URI: {tracking_uri}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    warmup_steps=500,
    weight_decay=0.02,
    logging_dir='./logs',
    logging_steps=5,  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è loss
                      # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ logging_steps=1
    eval_strategy="epoch",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: evaluation_strategy -> eval_strategy
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    save_total_limit=2,
    learning_rate=2e-5,
    fp16=torch.cuda.is_available(),  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ GPU
    report_to="none",  # –û—Ç–∫–ª—é—á–∞–µ–º wandb/tensorboard, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ MLFlow
)

# –°–æ–∑–¥–∞–Ω–∏–µ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[
        EarlyStoppingCallback(early_stopping_patience=2),
        MLFlowLoggingCallback()  # –î–æ–±–∞–≤–ª—è–µ–º callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –±–∞—Ç—á–∞–º
    ]
)

# –ù–∞—á–∞–ª–æ MLFlow run
print("\n" + "="*60)
print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
print("="*60)

with mlflow.start_run(run_name=f"finbert_emotion_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    mlflow.log_params({
        "model_name": "ProsusAI/finbert",
        "num_labels": num_labels,
        "num_train_samples": len(train_df),
        "num_val_samples": len(val_df),
        "num_test_samples": len(test_df),
        "max_length": 128,
        "learning_rate": training_args.learning_rate,
        "batch_size": training_args.per_device_train_batch_size,
        "num_epochs": training_args.num_train_epochs,
        "weight_decay": training_args.weight_decay,
        "warmup_steps": training_args.warmup_steps,
    })
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    mlflow.log_dict(
        {str(i): label for i, label in enumerate(label_encoder.classes_)},
        "label_mapping.json"
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    train_result = trainer.train()
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    mlflow.log_metrics({
        "train_loss": train_result.training_loss,
        "train_runtime": train_result.metrics.get('train_runtime', 0),
        "train_samples_per_second": train_result.metrics.get('train_samples_per_second', 0),
    })
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ
    print("\n–û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ...")
    val_metrics = trainer.evaluate()
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    mlflow.log_metrics({
        "val_loss": val_metrics['eval_loss'],
        "val_accuracy": val_metrics['eval_accuracy'],
        "val_f1": val_metrics['eval_f1'],
        "val_precision": val_metrics['eval_precision'],
        "val_recall": val_metrics['eval_recall'],
    })
    
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
    print(f"   Loss: {val_metrics['eval_loss']:.4f}")
    print(f"   Accuracy: {val_metrics['eval_accuracy']:.4f}")
    print(f"   F1: {val_metrics['eval_f1']:.4f}")
    print(f"   Precision: {val_metrics['eval_precision']:.4f}")
    print(f"   Recall: {val_metrics['eval_recall']:.4f}")
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    print("\n–û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")
    test_metrics = trainer.evaluate(eval_dataset=test_dataset)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ç–µ—Å—Ç–∞
    mlflow.log_metrics({
        "test_loss": test_metrics['eval_loss'],
        "test_accuracy": test_metrics['eval_accuracy'],
        "test_f1": test_metrics['eval_f1'],
        "test_precision": test_metrics['eval_precision'],
        "test_recall": test_metrics['eval_recall'],
    })
    
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Ç–µ—Å—Ç–∞:")
    print(f"   Loss: {test_metrics['eval_loss']:.4f}")
    print(f"   Accuracy: {test_metrics['eval_accuracy']:.4f}")
    print(f"   F1: {test_metrics['eval_f1']:.4f}")
    print(f"   Precision: {test_metrics['eval_precision']:.4f}")
    print(f"   Recall: {test_metrics['eval_recall']:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    print("\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º...")
    test_predictions = trainer.predict(test_dataset)
    test_pred_labels = np.argmax(test_predictions.predictions, axis=1)
    test_true_labels = test_predictions.label_ids
    
    class_report = classification_report(
        test_true_labels,
        test_pred_labels,
        target_names=label_encoder.classes_,
        output_dict=True
    )
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
    for emotion, metrics in class_report.items():
        if isinstance(metrics, dict):
            mlflow.log_metrics({
                f"test_{emotion}_precision": metrics.get('precision', 0),
                f"test_{emotion}_recall": metrics.get('recall', 0),
                f"test_{emotion}_f1": metrics.get('f1-score', 0),
                f"test_{emotion}_support": metrics.get('support', 0),
            })
    
    print("\n" + classification_report(
        test_true_labels,
        test_pred_labels,
        target_names=label_encoder.classes_
    ))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n" + "="*60)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å pickle –∏ fp16: unwrap –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    try:
        from accelerate import unwrap_model
        # Unwrap –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ—Ç –æ–±–µ—Ä—Ç–∫—É –æ—Ç mixed precision)
        model_to_save = unwrap_model(trainer.model)
        print("‚úÖ –ú–æ–¥–µ–ª—å unwrapped —Å –ø–æ–º–æ—â—å—é accelerate")
    except (ImportError, AttributeError):
        # –ï—Å–ª–∏ accelerate –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±–µ—Ä–Ω—É—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é
        model_to_save = trainer.model
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DataParallel –∏–ª–∏ DistributedDataParallel
        if hasattr(model_to_save, 'module'):
            model_to_save = model_to_save.module
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ accelerate (–Ω–æ accelerate –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω)
        if hasattr(model_to_save, '_orig_mod'):
            model_to_save = model_to_save._orig_mod
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –±–µ–∑ accelerate")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ MLFlow (–∏—Å–ø–æ–ª—å–∑—É–µ–º unwrapped –º–æ–¥–µ–ª—å)
    try:
        mlflow.pytorch.log_model(
            model_to_save,
            "model",
            registered_model_name="finbert_emotion_classifier"
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ MLFlow")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ MLFlow: {e}")
        print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ (trainer.save_model –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç unwrap)
    trainer.save_model(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_SAVE_PATH}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ label encoder
    with open(os.path.join(MODEL_SAVE_PATH, 'label_encoder.pkl'), 'wb') as f:
        pickle.dump(label_encoder, f)
    print(f"‚úÖ Label encoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    mlflow.log_artifacts(MODEL_SAVE_PATH, "model_files")
    
    print("\n" + "="*60)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*60)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_SAVE_PATH}")
    print(f"MLFlow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {MLFLOW_EXPERIMENT_NAME}")
    print(f"–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: mlflow ui")

